
\chapter{New Equations for Neutral Terms}

\section{Introduction}

\newcommand{\fbc}[1]{\framebox{\hspace*{-0.04in}\texttt{\raisebox{0in}[0.10in][0.04in]{#1}}\hspace*{-0.04in}}}
\newcommand{\fbb}[1]{\framebox{\hspace*{-0.04in}\texttt{\raisebox{0in}[0.08in][0.02in]{#1}}\hspace*{-0.04in}}}
\newcommand{\fba}[1]{\framebox{\hspace*{-0.04in}\texttt{\raisebox{0in}[0.06in][0in]{#1}}\hspace*{-0.04in}}}
\DefineVerbatimEnvironment%
  {Code}{Verbatim}
  {
   commandchars=\\\{\}, codes={\catcode`$=3}
  }

The programmer working in intensional type theory is no stranger to
`obviously true' equations she wishes held \emph{definitionally} for
her program to typecheck without having to chase down ill-typed
terms and brutally coerce them. In this article, we present one way
to relax definitional equality, thus accomodating some of her
longings.

The first set of equalities~(table \ref{table:deltaiota}) one expects is introduced by the equations
the programmer writes to define functions; they correspond to $\delta$ (for
\emph{definitions}) and $\iota$ (for pattern-matching on \emph{inductive} data)
rules and hold computationally just like the more common $\beta$
rule. Definitional equality extends these rules to open terms and identifies
expressions up to computation.
\begin{table}[h]
\begin{Code}
map : (a $\rightarrow$ b) $\rightarrow$ list a $\rightarrow$ list b
map f []        $\mapsto$ []
map f (x :: xs) $\mapsto$ f x :: map f xs

(+\!+) : list a $\rightarrow$ list a $\rightarrow$ list a
[]      +\!+ ys $\mapsto$ ys
x :: xs +\!+ ys $\mapsto$ x :: (xs +\!+ ys)

fold : (a $\rightarrow$ b $\rightarrow$ b) $\rightarrow$ b $\rightarrow$ list a $\rightarrow$ b
fold c n []        $\mapsto$ n
fold c n (x :: xs) $\mapsto$ c x (fold c n xs)
\end{Code}
\caption{\label{table:deltaiota}$\delta\iota$ rules - computational}
\end{table}

The second batch~(table \ref{table:eta}) explains that some types have unique constructors
which the programmer can demand. These are usually
called $\eta$ rules and articulate some degree of \emph{extensionality}. They are
well supported in e.g. Epigram~\cite{DBLP:conf/sfp/ChapmanAM05} and \agda~\cite{agda}
both for functions and records but still lacking for records in \coq~\cite{coq}.
\begin{table}[h]
\begin{Code}
f ≡ $\lambda$ x. f x       : a $\to$ b
p ≡ ($\piun$ p , $\pide$ p)  : a * b
u ≡ ()             : 1
\end{Code}
\caption{\label{table:eta}$\eta$ rules - canonicity}
\end{table}

%The last set of equalities corresponds to the kind of facts Boyer and Moore's
%recursion analysis~\cite{BoyerMoore} can discover. We name these rules $\nu$
%(for \emph{neutrals}) given that, because of their Boyer-Moore nature, they
%will hold true for the whole calculus whenever they do on neutral
%terms.
The free variables of open terms obstruct those computation rules
which require constructors, hence those rules thus determine a
kit for building computationally inert \emph{neutral} terms, growing
layers of thwarted progress around a variable which we dub the `nut'.
\[
\fba{x}\;\;\fbb{\fba{~}~a}\;\;\fbb{$\piun$~\fba{~}}\;\;\fbb{$\pide$~\fba{~}}
\;\;\fbb{\fba{~}~+\!+~ys}\;\;\fbb{map~f~\fba{~}}\;\;\fbb{fold~n~c~\fba{~}}
\]
Our new `$\nu$ rules'~(table \ref{table:nu}) concern just such neutral
terms, with the same nut obstructing both sides. Each can be
proven just by structural induction on the nut, cracking it into
constructor cases which compute by $\beta\delta\iota$ to subgoals
which follow by inductive hypothesis---the classic proof pattern of
Boyer and Moore~\cite{BoyerMoore}.
\begin{table}[h]
%\begin{Code}
%xs +\!+ [] = xs
%xs +\!+ (ys +\!+ zs) = (xs +\!+ ys) +\!+ zs
%
%map f (xs +\!+ ys) = map f xs +\!+ map f ys
%map f (map g xs) = map (f . g) xs
%
%fold c n (map f xs) = fold (c . f) n xs
%fold c n (xs +\!+ ys) = fold c (fold c n ys) xs
%\end{Code}
\begin{tabular}{@{}rcl@{}}
\fbb{\fba{xs}~+\!+~[]} & = & \fba{xs} \smallskip\\
\fbc{\fbb{(\fba{xs}~+\!+~ys)}~+\!+~zs} & = & \fbb{\fba{xs}~+\!+~(ys~+\!+~zs)}
\medskip\\
\fbb{map~id~\fba{xs}} & = & \fba{xs} \smallskip\\
\fbc{map~f~\fbb{(map~g~\fba{xs})}} & = & \fbb{map~(f~.~g)~\fba{xs}}
\medskip \\
\fbc{map~f~\fbb{(\fba{xs}~+\!+~ys)}} & = & \fbc{\fbb{map~f~\fba{xs}}~+\!+~map~f~ys}
\medskip \\
\fbc{fold~c~n~\fbb{(map~f~\fba{xs})}} & = & \fbb{fold~(c~.~f)~n~\fba{xs}}\smallskip\\
\fbc{fold~c~n~\fbb{(\fba{xs}~+\!+~ys)}} & = & \fbb{fold~c~(fold~c~n~ys)~\fba{xs}}
\end{tabular}
\caption{\label{table:nu}$\nu$ rules}
\end{table}

%It is no mystery that the equational theory generated from the $\beta\iota$ rules
%is decidable\todo{cite}. The reader probably also knows extensions of these
%procedures accomodating $\eta$ rules\todo{cite}.
Here, we give a decision procedure for
$\rules$-equality with an \agda\ proof that it is
sound and complete\footnote{\url{https://patch-tag.com/r/gallais/icfp2013}}.
We gain, for example, that \texttt{map swap . map swap = id}, where \texttt{swap}
swaps the elements of a pair. Implementing $\nu$-rules separately from evaluation
and only for neutral terms makes them none the less admissible in general. This
promising experiment suggests that other frustrating equations of a similar
character may soon come within our grasp.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%% Setting %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Our Experimental Setting}

Our ambition is to provide a generic method of extending the
definitional equality of intensional type theories with $\nu$-rules,
but here we deliver evidence of progress in a simpler setting which is
easier to formalize: simply typed $\lambda$-calculus with products and
list primitives. We developed the algorithm during Boutillier's internship at
Strathclyde~\cite{LambList}; Allais completed the formalized metatheory.

\paragraph{Types} are parametrized by a natural number $n$ and can refer to a
finite set of base types using the $\tybase[]$ constructor and an index $k$ in
$\fin$, the datatype with exactly $n$ inhabitants. These
unanalysed base types give us a simple way to model expressions
exhibiting some parametric polymorphism.
$$\sigma, \tau, \dots \gramdecl
           \tybase
  \mid \tyunit
  \mid \typrod
  \mid \tyarrow
  \mid \tylist$$

\paragraph{Terms} follow the grammar presented below and the typing rules
described in the figure page~\pageref{typingrules} where contexts are just
snoc lists of variable names together with their type.
\begin{align*}
 t, u, \dots
    & \gramdecl x \mid \telam x. t \mid t \teapp u \mid \tett \mid t \tepair u \mid \tepiun t \mid \tepide t \mid \tenil \\
    & \mid hd \tecons tl \mid \temap(f , xs) \mid xs \teappend ys \mid \tefold (c , n , xs)
\end{align*}
\begin{figure*}
\begin{tabular}{l|r}
\begin{minipage}{0.25\textwidth}
\begin{mathpar}
\inferrule{ }{\base \colon \incl[\ConEmpty][\ConEmpty]}
\and \inferrule{\mathit{pr} \colon \incl}
     {\pop  \mathit{pr} \colon \incl[\protect{\ConExtend[\Gamma][(x \colon \sigma)]}][\protect{\ConExtend[\Delta][(x \colon \sigma)]}]}
\and \inferrule{\mathit{pr} \colon \incl}
     {\step  \mathit{pr} \colon \incl[\Gamma][\protect{\ConExtend[\Delta][(x \colon \sigma)]}]}
\end{mathpar}
\end{minipage}

& \begin{minipage}{0.75\textwidth}\begin{mathpar}
\inferrule{(x \colon \sigma) \in \Gamma}{\typingrule[x]}
\and \inferrule{\typingrule[t][\tau][\protect{\ConExtend[\Gamma][(x \colon \sigma)]}  ]}{\typingrule[\telam x. t][\tyarrow]}
\and \inferrule{\typingrule[t][\tyarrow] \\ \typingrule[u]}{\typingrule[t \teapp u][\tau]}
\and \inferrule{ }{\typingrule[\tett][\tyunit]}
\and \inferrule{\typingrule[t] \\ \typingrule[u][\tau]}{\typingrule[t \tepair u][\typrod]}
\and \inferrule{\typingrule[t][\typrod]}{\typingrule[\tepiun t]}
\and \inferrule{\typingrule[t][\typrod]}{\typingrule[\tepide t][\tau]}
\and \inferrule{ }{\typingrule[\tenil][\tylist]}
\and \inferrule{\typingrule[hd] \\ \typingrule[tl][\tylist]}{\typingrule[hd \tecons tl][\tylist]}
\and \inferrule{\typingrule[f][\tyarrow] \\ \typingrule[xs][\tylist]}{\typingrule[\temap(f , xs)][\tylist[\tau]]}
\and \inferrule{\typingrule[c][\tyarrow[\tyarrow]] \\ \typingrule[n][\tau] \\ \typingrule[xs][\tylist]}{\typingrule[\tefold(c , n , xs)][\tau]}
\end{mathpar}
\end{minipage}
\end{tabular}
\label{typingrules}
\caption{Context inclusion and typing rules}
\end{figure*}
For the sake of clarity in formalization, we quote the constructors of
our object language, making a clear distinction from the corresponding
features of the host language, \agda, where we use the standard `typed
de Bruijn index' representation of well typed
terms~\cite{deBruijn:dummies,DBLP:conf/csl/AltenkirchR99} to eliminate
junk from consideration.
In our treatment here, we always assume freshness of the variables
introduced by lambdas and we do not artificially separate terms and typing
derivations.

The notion of context inclusion is defined inductively and gives rise to
a weakening operation essentially acting on variables. The usual results
about the identity weakening, composition of weakenings, etc. hold.

The pointwise extension of the well-typed terms from types to contexts gives
a notion of parallel substitution.
%All the properties described in this document
%onwards can be naturally extended in a pointwise fashion to parallel substitutions
%and this extension is usually referred to by adding a $\varepsilon$ to the name of
%the property.
$$\subst = \left\lbrace
\begin{array}{l@{\text{ if }\Gamma =~}l}
\top & \varepsilon \\
\subst[\Gamma'] \times \term[\sigma][\Delta] & \Gamma' \cdot \sigma
\end{array}\right.$$
We write $t [ \rho ]$ for the application of the parallel substitution $\rho \colon \subst$
to the term $t \colon \term$ yielding a term of type $\term[\sigma][\Delta]$.

%\begin{remark}[Remarks about the formalization:]
%\par When writing typing derivations, we always assume freshness of the variables
%introduced by lambdas. This is dealt with in the formalization by using de Bruijn
%indices. Moreover it is easier in \agda{}'s setting to work directly on terms
%well-typed by construction given that most of the proofs are done by induction on
%the typing derivation. Therefore we do not artificially separate terms and typing
%derivations.
%\end{remark}

\paragraph{The equational theory} of the calculus, denoted $\conv$, is quite
naturally the congruence closure of the $\rules$-rules described earlier where
reductions under $\lambda$-abstraction are allowed. In this paper, we also mention
the relation $\reduces$ where the rules presented earlier are all considered with
a left to right orientation (except for the identity laws for the list functor
and the list monoid) thus inducing a notion of \emph{reduction}. The soundness
theorem proves that not only is the term produced related to the source one but
it is a reduct of it.

One easy sanity check we recommend before starting to work on the metatheory was
to give a shallow embedding of the calculus in a pre-existing sound type theory
and to show that the reduction relation is compatible with the propositional
equality in this theory. We used \agda{} extended with a postulate stating
extensional equality for non-dependent functions in our formalization. Once the
reader is satisfied that no silly mistakes were made in the equational theory,
she can start the implementation.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%% Setting %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Reduction machinery}

When looking in details at different accounts of normalization by evaluation
~\cite{BerSch91,CoqDyb97,Coquand02,NbeEffects}, the reader should be able to
detect that there are two phases in the process:
firstly the evaluation function building elements of the model from well-typed
terms performs $\beta\delta\iota$-reductions and does not reduce under
$\lambda$-abstractions effectively building closures --using the $\lambda$-abstractions
of the host language-- when encountering one.
Secondly the quoting machinery extracting terms from the model performs $\eta$-expansions
where needed which will cause the closures to be reduced and new computations to
be started. This two steps process was already more or less present in Berger and
Schwichtenberg's original paper~\cite{BerSch91}:
\begin{quote}Obviously each term in $\beta$-normalform may be transformed into
long $\beta$-normalform by suitable $\eta$-expansions. Therefore each term $r$
may be transformed into a unique long $\beta$-normalform $r^\star$ by
$\beta$-conversion and $\eta$-expansions.
\end{quote}

Building on this ascertainment, we construct a three (rather than two) staged
process successively performing $\beta\delta\iota$, $\eta$ and finally $\nu$
reductions whilst always potentially calling back a procedure from a preceeding
stage to reduce further non-normal terms appearing when e.g. going under lambdas
during $\eta$-expansion, distributing a map over an append, etc.

\subsection{The three stages of standardization}
\label{bigstep}
The normalization and standardization process goes through three succesive stages
whence the need to define three different subsets of terms of our calculus. They
have to be understood simply as syntactic category restricting the shape of terms
typed in the same way as the ones in the original languages except for the few
extra constructors for which we explicitly detail what they mean.

\begin{remark}It should be noted that the two last steps never reduce a term to
a constructor-headed one for datatypes (lists in our setting). In particular,
the last step only rearranges stuck terms to produce terms which are themselves
stuck. In other words: if a (list in our case) term is convertible to a constructor
headed term (be it either nil or cons), then it is reduced to it by the first step
of the reduction.
\end{remark}

The first intermediate language we are going to encounter is composed of weak-head
$\beta\delta\iota$-normal expressions i.e. we never reduce under a lambda, this
role being assigned to the $\eta$-expansion routine. Having $\lambda$-closures as
first-class values is one of the marks of this approach.
\begin{empheq}[outerbox=\fbox]{align*}
& \texttt{Weak-head normal forms} \\
m \gramdecl & x \mid m \teapp w \mid \tepiun m \mid \tepide m \mid \tefold (w_1 , w_2 , m) \\
            & \mid \temap(w , m) \mid m \teappend w \\
w \gramdecl & m \mid \telamclosure x. t \mid \tett \mid w_1 \tepair w_2 \mid \tenil \mid w_1 \tecons w_2\\
\rho \gramdecl & \varepsilon \mid \rho , x \mapsto w
\end{empheq}
These values are computed using a simple off the shelf environment machine which
returns a constructor when facing one; stores the evaluation environment in a
$\lambda$-closure when evaluating a term starting with a $\telam$; and calls an
helper function on the recursively evaluated subterms when uncovering an eliminator.
These helper functions either return a neutral if the interesting subterm was
stuck or perform the elimination which may start new computations (e.g. in the
application case).
\begin{remark}This reduction step is absolutely type-agnostic and could therefore
be performed on terms devoid of any type information as in e.g. \coq{} where
conversion is untyped. Keeping and propagating \emph{some} types (e.g. the codomain
of the funcion in a map) is nonetheless needed to be able to infer back the type
of the whole expression which is crucial in the following steps.
\end{remark}
Then an $\eta$-expansion step kicks in and produces $\eta$-long values in a
type-directed way. It insists that the only neutrals worthy of being considered
normal forms are the ones of the base type. It also carves out the subset of stuck
lists in a separate syntactic category $l$ thus preparing for the last step which
will leave most of the rest of the language untouched.

The $\eta$-expansion of product and function type actually calls back the subroutines
for $\beta\delta\iota$-rules respectively in charge of computing the first and
second projection of a pair and the application of a function to a term (here the
variable newly introduced). This step is the only one requiring a name generator
which allows us to avoid threading such an artifact along the whole reduction
machinery.
\begin{empheq}[outerbox=\fbox]{align*}
& \texttt{$\eta$-long values} \\
n \gramdecl & x \mid n \teapp v \mid \tepiun n \mid \tepide n \mid \tefold (v_1 , v_2 , l) \\
v \gramdecl & n_{\tybase} \mid l \mid \telam x. v \mid \tett \mid v_1 \tepair v_2 \mid \tenil \mid v_1 \tecons v_2 \\
l \gramdecl & n_{\tylist} \mid \temap (v , l) \mid l \teappend v
\end{empheq}
Standard forms have a very specific shape due to the fact that we now completely
internalize the $\nu$-rules. The standard lists $s$ are produced by flattening
the stuck map / append trees present in $l$ after the end of the previous
procedure whilst the fold / map and fold / append fusion rules are applied in
order to compute folds further and reach the point where a stuck fold is stuck on
a \emph{real} neutral lists.
\begin{empheq}[outerbox=\fbox]{align*}
& \texttt{standard forms} \\
n \gramdecl & x \mid n \teapp v \mid \tepiun n \mid \tepide n \mid \tefold (v_1 , v_2 , n) \\
v \gramdecl & n_{\tybase} \mid s \mid \telam x. v \mid \tett \mid v_1 \tepair v_2 \mid \tenil \mid v_1 \tecons v_2 \\
s \gramdecl & \temappend
\end{empheq}
The new constructor $\temappend[\_][\_][\_]$ --refered to as "mappend"-- has the
obvious semantics that it represents the concatenation of a stuck map and a list.
This grammar explicitly defines a hierarchy between stuck functions: appends are
forbidden to appear inside maps and both of them have better not be found sitting
in a fold. It is but one way to guarantee the existence of standard forms and
future extensions hopefully allowing the programmer to add the $\nu$-rules she
fancies holding definitionally will have to make sure --for completeness' sake--
that such standard forms exist.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% Formalization %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Formalization of the procedure}

What we are interested in here is to demonstrate the decidability of the equational
theory's extension rather than explaining how to prove termination of a big step
semantics in \agda{} and rely on functional induction to prove the different
properties. The reader keen on learning about the latter should report to James
Chapman's thesis~\cite{ChapmanPhd} where he describes a principled solution to
proving termination of big step semantics for various calculi. We, on the other
hand, will focus on the former: we opted for a version of the algorithm based, in
the tradition of normalization by evaluation, on a model construction which
basically collapses the layered stages but is trivially terminating by a structural
argument.

\paragraph{Type directed partial evaluation} (or normalization by evaluation) is
a way to compute the canonical forms by using the evaluation mechanism of the host
language whilst exploiting the available type information to retrieve terms from
the semantical objects. It was introduced by Berger and Schwichtenberg~\cite{BerSch91}
in order to have an efficient normalization procedure for \lego{}. It has since been
largely studied in different settings:

Danvy's lecture notes~\cite{TypeDirected} review its foundations and presents its
applications as a technique to get rid of static redexes when compiling a program.
It also discusses various refinements of the naïve approach such as the introduction
of let bindings to preserve a call-by-value semantics or the addition of extra
reduction rules\footnote{E.g. $n + 0 \leadsto n$ in a calculus where $\_+\_$ is
defined by case analysis on the first argument and this expression is therefore
stuck.} to get cleaner code generated. Our $\nu$-rules are somehow reminiscent of
this approach.

T. Coquand and Dybjer~\cite{CoqDyb97} introduced a glued model recording the partial
application of combinators in order to be able to build the reification procedure
for a combinatorial logic. In this case the naive approach is indeed problematic
given that the \texttt{SK} structure is lost when interpreting the terms in the
naïve model and is impossible to get back. This was of great use in the design of
a model outside the scope of this paper computing only weak-head normal forms~\cite{ForgeAcquire}.

C. Coquand~\cite{Coquand02} showed in great details how to implement and prove sound
and complete an extension of the usual algorithm to a simply-typed lambda calculus
with explicit substitutions. This development guided our correctness proofs.

More recently Abel et al.~\cite{NbeDep1, NbeDep2} built extensions able to deal with
a variety of type theories and last but not least Ahman~\cite{NbeEffects} explained
how to treat calculi equipped with algebraic effects.

\begin{remark}We will call $\normal$ the typing derivations restricted to standard
values as per the previous section's definitions and $\neutral$ the corresponding
ones for standard neutrals. Standard list will be silently embedded in standard
values: the separation of $s$ and $v$ is an important vestige of the syntactic
category $l$ of stuck lists but inlining it in the grammar yields exactly the
same set of terms.
\end{remark}

\begin{definition} The model is defined by induction on the type using an auxiliary
inductive definition parametric in its arguments --which guarantees that the
definition is strictly positive therefore meaningful-- to give a semantical account
of lists. One should remember that the calculus enjoys $\eta$-rules for unit,
product and arrow types; therefore the semantical counterpart of terms with such
types need not be more complex than unit, pairs and actual function spaces.
$$\begin{array}{@{\NBE(\Gamma,~}l@{~)~}cl}
  . & : & \type \rightarrow \set \\
  \tyunit  & = & \top\\
  \tybase  & = & \neutral[\tybase]\\
  \typrod  & = & \model[\sigma] \times \model[\tau]\\
  \tyarrow & = & \forall \Delta, \incl \rightarrow \model[\sigma][\Delta] \rightarrow \model[\tau][\Delta]\\
  \tylist  & = & \modellist[\sigma][\modellistfun]\\
\end{array}$$
Standardization may trigger new reductions and we have therefore the obligation
to somehow store the computational power of the functions part of stuck maps. This
is a bit tricky because the domain type of such functions is nowhere related to
the overall type of the expression meaning that no induction hypothesis can be used.
Luckily these new computations are only ever provoked by neutral terms: they come
from function compositions caused by map or map-fold fusions.
\begin{mathpar}
\inferrule{\Gamma \colon \context \\ \sigma \colon \type \\ \modellistfunabs \colon \context \rightarrow \set}
{\modellist \colon \set}
\and \inferrule{ }{\tenil : \modellist}
\and \inferrule{\mathit{HD} \colon \modellistfunabs(\Gamma) \\ \mathit{TL} \colon \modellist}
{\mathit{HD} \tecons \mathit{TL} \colon \modellist}
\and \inferrule{\mathit{F} \colon \forall \Delta, \incl \rightarrow \neutral[\tau][\Delta] \rightarrow \modellistfunabs(\Delta) \\
\mathit{xs} \colon \neutral[\protect{\tylist[\tau]}] \\ \mathit{YS} \colon \modellist}
{\temappend[\mathit{F}][\mathit{xs}][\mathit{YS}] \colon \modellist}
\end{mathpar}
\end{definition}
\begin{remark}One should notice the Kripke flavour of the interpretation of
function types. It is exactly what is needed to write down a weakening operation
thus giving the entire model a Kripke-like structure.
\end{remark}
\begin{lemma}[Reify and reflect] Mutually defined processes allow normal forms
$\normal$ to be extracted from elements of the model $\model$ whilst neutral
forms $\neutral$ can be turned into elements of the model.
\end{lemma}
\begin{proof} Both $\reify \colon \model \rightarrow \normal$ and $\reflect \colon
\neutral \rightarrow \model$ are defined by induction on their type index $\sigma$.
The unit case is trivial: the reification process returns $\tett$ while the
reflection one produces the only inhabitant of $\top$. The base type case is
solved by the embedding of neutrals into normals on one hand and by the identity
function on the other hand. The product case is simply discharged by invoking
the induction hypotheses: the reification is the pairing of the reifications of
the subterms while the reflection is the reflection of the $\eta$-expansion of
the stuck term. We can now focus on the more subtle cases.

The function case is obtained by $\eta$-expansion both at the term level (the
normal form will start with a $\telam$) and the semantical level (the object
will be a function). It is here that the fact that the definitions are mutual
is really important.
\begin{align*}
\reify[\tyarrow] F &= \telam x. \reify[\tau] F (\_, \reflect x) \\
\reflect[\tyarrow] f &= \lambda \Delta~inc~x.\reflect[\tau](\weaken(f) \teapp \reify[\sigma] x)
\end{align*}
The list case is dealt with by recursion on the semantical list for the reification
process and a simple injection for the reflection case. We write $\listreify$ and
$\listreflect$ for the helper functions performing reification and reflection on
lists of type $\tylist$.
$$\begin{array}{@{\listreify}l@{~=~}l}
\tenil                                  & \tenil \\
\mathit{HD} \tecons \mathit{TL}         & \reify \mathit{HD} \tecons \listreify \mathit{TL} \\
\temappend[f][\mathit{xs}][\mathit{YS}] & \temappend[\telam x.\reify f(x)][\mathit{xs}][\listreify \mathit{YS}]
\end{array}$$
This injection corresponds to applying the identity functor and monoid law. Indeed
$\lambda \Delta \_. \reflect$ denotes the identity function and has the appropriate
type $\forall \Delta, \incl \rightarrow \neutral[\sigma][\Delta] \rightarrow \modellistfunabs(\Delta)$
to fit in the semantical list mappend constructor.
$$\listreflect xs ~=~ \temappend[\lambda \Delta \_. \reflect][\mathit{xs}][\tenil]$$
\end{proof}

\begin{example} of $\eta\nu$-expansions provoked by the reflect / reify functions:
for $\mathit{xs}$ a neutral list of type $\gdef\pub{\typrod[\tyunit][\hspace{-3pt}\tybase]}\def\Tpub{\tylist[(\pub)]}\Tpub$,
we get an expanded version by drowning it in the model and reifying it back:
$$\listreify[\pub] (\listreflect[\pub] \mathit{xs}) = \temappend[\protect{\telam p. (\teunit \tepair \tepide p)}][\mathit{xs}][\tenil]$$
This showcases the $\eta$-expansion of unit, products and functions as well as the
use of the identity laws mentionned during the definition of $\listreflect$.
\end{example}

Proving that every term can be normalized now amounts to proving the existence of
an evaluation function producing a term $T$ of the model $\model[\sigma][\Delta]$
given a well-typed term $t$ of the language $\term$ and a semantical environment
$\modele$. Indeed the definition of the reflection function $\reflect$ together
with the existence of environment weakenings give us the necessary machinery to
produce a diagonal semantical environment $\modele[\Gamma][\Gamma]$ which could
then be fed to such an evaluation function.

In order to keep the development tidy and have a more modular proof of correctness,
it is wise to give this evaluation function as much structure as possible. This is
done through a multitude of helper functions explaining what the semantical
counterparts of the usual combinators of the calculus (except for lambda which,
integrating a weakening to give the model its Kripke structure, is a bit special)
ought to look like.
\begin{figure*}
\small
\begin{tabular}{lr}
\begin{minipage}{0.45\textwidth}$$
\begin{array}{l@{~\vappend\mathit{ZS}=\,}l}
\tenil                          & \mathit{ZS} \\
\mathit{HD} \tecons \mathit{TL} & \mathit{HD} \tecons (\mathit{TL} \vappend \mathit{ZS}) \\
\temappend[F][\mathit{xs}][\mathit{YS}]
                                & \temappend[\mathit{F}][\mathit{xs}][(\mathit{YS} \vappend \mathit{ZS})]
\end{array}$$

$$\begin{array}{@{\vmap~\mathit{F}~}l@{~=~}l}
\tenil                          & \tenil \\
(\mathit{HD} \tecons \mathit{TL}) & F (\_, \mathit{HD}) \tecons \vmap F \, \mathit{TL} \\
(\temappend[G][\mathit{xs}][\mathit{YS}])
                                & \temappend[F \vcomp G][\mathit{xs}][\vmap F \, \mathit{YS}] \\
\multicolumn{2}{l}{\text{where}~ F ~\vcomp~ G = \lambda E~\mathit{inc}~t. F (\mathit{inc}, G(\mathit{inc},t))}
\end{array}$$

$$\begin{array}{l@{~=~}l}
\vfold~~C~N~\tenil                            & N \\
\vfold~~C~N~(\mathit{HD} \tecons \mathit{TL}) & C (\_,\mathit{HD},\_, \vfold~C~N~\mathit{TL}) \\
\vfold_{\tau} C~N~(\temappend[F][\mathit{xs}][\mathit{YS}])
                                  & \reflect[\tau] \tefold (c , n , \mathit{xs}) \\
\multicolumn{2}{l}{
\begin{array}{ll@{~=~}l}
\text{where} & c & \telam x. \telam y. \reify[\tau] C(\_, F(\_, x)), \_ , \reflect[\tau] y) \\
             & n & \reify[\tau] \vfold~C~N~YS
\end{array}}
\end{array}$$
\end{minipage}
& \begin{minipage}{0.55\textwidth}$$
\begin{array}{@{\termeval~}l@{~R~=~}l}
(\tevar pr    )& R \lookup pr \\
(\telam x. t  )& \lambda E~\mathit{inc}~S. \termeval~t ~(\weakene(R) , x \mapsto S) \\
(f \teapp x   )& (\termeval~f~R) (\_, \termeval~x~R) \\
(\tett        )& \teunit \\
(a \tepair b  )& \termeval~a~R, \termeval~b~R\\
(\tepiun t    )& \piun (\termeval~t~R) \\
(\tepide t    )& \pide (\termeval~t~R) \\
(\tenil       )& \tenil \\
(\mathit{hd} \tecons \mathit{tl})& (\termeval~\mathit{hd}~R) \tecons (\termeval~\mathit{tl}~R) \\
(\mathit{xs} \teappend \mathit{ys})& (\termeval~\mathit{xs}~R) \vappend (\termeval~\mathit{ys}~R) \\
(\temap (f , \mathit{xs}) )& \vmap (\termeval~f~R) (\termeval~\mathit{xs}~R) \\
(\tefold (c , n , \mathit{xs}) )& \vfold (\termeval~c~R) (\termeval~n~R) (\termeval~\mathit{xs}~R)
\end{array}$$
\end{minipage}
\end{tabular}
\label{evalfun}
\caption{Evaluation function and helper functions}
\end{figure*}
\begin{theorem}[Evaluation function] Given a term in $\term$ and a semantical
environment in $\modele$, one can build a semantical object in $\model[\sigma][\Delta]$.
\end{theorem}
\begin{proof}A simple induction on the term to be evaluated using the semantical
counterparts of the calculus' combinators to assemble semantical objects obtained
by induction hypotheses discharges most of the goals. See the figure page~\pageref{evalfun}
for the details of the code.

In the lambda case, we have the body of the lambda $b$ in $\term[\tau][\Gamma \cdot \sigma]$,
an evaluation environment $R$ in $\modele$ and we are given a context $E$, a proof
\texttt{inc} that $\incl[\Delta][E]$ and an object $S$ living in $\model[\sigma][E]$.
By combining $S$ and a weakening of $R$ along \texttt{inc}, we get an evaluation
environment of type $\modele[\Gamma \cdot \sigma][E]$ which is just what we needed
to conclude by using the $\model[\tau][E]$ delivered by the induction hypothesis
on $b$.
\end{proof}

\begin{remark}The only place where type information is needed is when reorganizing
neutrals following $\nu$-rules e.g. in the semantical fold. The evaluation function
is therefore faithful to the staged evaluation approach.

The model is indeed related to the algorithm presented earlier on in section~\ref{bigstep}:
we \emph{describe} all the computations eagerly for \agda{} to see the termination
argument but a subtle evaluation strategy applied to the produced code could reclaim
the behaviour of the layered approach. It would have to form lambda closures in
the arrow case, fire eagerly only the reductions eliminating constructors in
$\vmap$, $\vappend$ and $\vfold$ helper functions thus postponing the execution
of the code corresponding to $\eta\nu$-rules to reification time.
\end{remark}

\begin{corollary}There is a normalization function $\norm$ turning terms in
$\term$ into normal forms in $\normal$.
\end{corollary}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%% Correctness %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Correctness}

The typing information provided by the implementation language guarantees that
the procedure computes terms in normal forms from its inputs and that they have
the same type. This is undoubtly a good thing to know but does not forbid all
the potentially harmful behaviours: the empty list is a type correct normal form
for any input of type list but it certainly is not a satisfactory answer with
respect to $\rules$-equality. Hence the need for a soundness and a completeness
theorem tightening the specification of the procedure.

The meta-theory is an ad-hoc extension of the techniques already well explained
by Catarina Coquand~\cite{Coquand02} in her presentation of a simply-typed lambda
calculus with explicit substitutions (but no data). Soundness is achieved through
a simple logical relation while completeness needs two mutually defined notions
explaining what it means for elements of $\NBE$ to be semantically equal and to
behave uniformly on extensionally equal terms.

The reader should think of these logical relations as specifying requirements for
a characterization (being equal, being uniform) to be true of an element at some
type. The natural deduction style presentation of these recursive functions should
then be quite natural for her: read in a bottom-top fashion, they express that
the (dependent) conjunction of the hypotheses --the empty conjunction being $\top$--
is the requirement for the goal to hold. Hence leading to a natural interpretation:
\begin{mathpar}\inferrule{}{\inferrule{A \\ B \\ C}{F(t)} \\ {\huge\leadsto} \\ F(t) = A \times B \times C}\end{mathpar}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Soundness}

Soundness amounts to re-building the propositional part of the reducibility
candidate argument~\cite{Girard06} which has been erased to get the barebones
model. The logical relation $\sound$ relates a semantical object $T$ in $\model$
and a term $t$ in $\term$ which is morally the source of the semantical object.

\begin{definition}$\sound$ is defined by induction on the type $\sigma$ plus an
appropriate inductive definition for the list case. The unit and base type cases
are, as expected, the simplest ones and the product case is not very much more
exciting:
\begin{mathpar}
\inferrule{ }{\sound[\tyunit]}
\and \inferrule{t \reduces T}{\sound[\tybase]}
\end{mathpar}
$$\inferrule{a \colon \term \\ b \colon \term[\tau] \\ t \reduces a \tepair b
\\ \sound[\sigma][a][A] \\ \sound[\sigma][b][B]}{\sound[\typrod][t][A , B]}$$
Function types on the other hand give rise to a Kripke-like structure in two ways:
in addition to the quantification on all possible future context which we need to
match the model construction, there is also a quantification on all possible source
term reducing to the current one.
$$\inferrule{\forall \Delta (inc \colon \incl) ~ x ~ X, \sound[\sigma][x][X][\Delta] \rightarrow \\
\forall t \reduces \weaken f \teapp x, \sound[\tau][t][F (\mathit{inc} , X)][\Delta]}
{\sound[\tyarrow][f][F]}$$
It should be no surprise to the now experienced reader that the inductive definition
of the logical relation for $\tylist$ is parametrized by $\soundlistfunabs[.][.]$,
the interpretation of the relation for elements of type $\sigma$, simply to avoid
positivity problems. It is ultimately instantiated with the logical relation taken
at type $\sigma$.
$$\inferrule{\mathit{xs} \colon \term[\tylist] \\ \mathit{XS} \colon \modellist \\
\soundlistfunabs[.][.] \colon \forall \Gamma, \term \rightarrow \modellistfunabs \Gamma \rightarrow \set}{\soundlist \colon \set}$$
The cases for nil and cons are simply saying that the source term indeed reduces
to a term with the corresponding head-constructors and that the eventual subterms
are also related to the subobjects.
\begin{mathpar}
\inferrule{t \reduces \tenil}{\soundlist[t][\tenil]}
\and \inferrule{t \reduces \mathit{hd} \tecons \mathit{tl} \\
  \soundlistfunabs[\mathit{hd}][\mathit{HD}] \\ \soundlist[\mathit{tl}][\mathit{TL}]}
{\soundlist[\sigma][t][\mathit{HD} \tecons \mathit{TL}]}
\end{mathpar}
The map-append case is a bit more complex. The term is expected to reduce to a term
with the same canonical shape and then we expect the semantical function to behave
like the one discovered.
$$\inferrule{t \reduces \temap (f , \mathit{xs}) \teappend \mathit{ys} \\
\forall \Delta (\mathit{inc} \colon \incl) ~t \rightarrow \soundlistfunabs[\weaken(f) \teapp t][F (\mathit{inc} , t)] \\
\soundlist[\mathit{ys}][\mathit{YS}]}{\soundlist[t][\protect{\temappend[F][\mathit{xs}][\mathit{YS}]}]}$$
\end{definition}

The first thing to notice is that whenever two objects are related by this
logical relation then the property of interest holds true i.e. the semantical
object indeed is a reduct of the source term. This result which mentions the
reifying function has to be proven together with the corresponding one about
the mutually defined reflection function.

\begin{lemma}Reflect and reify are compatible with this logical relation in the
sense that:
\begin{enumerate}
  \item If $t_{\mathit{ne}}$ is a neutral $\neutral$ then $\sound[\sigma][t_{\mathit{ne}}][\reflect t_{\mathit{ne}}]$.
  \item If $t$ and $T$ are such that $\sound$ then $t \reduces \reify T$
\end{enumerate}
\end{lemma}

The Kripke-style structure we mentionned during the definition of the logical
relation adds just what is need to have it closed under anti-reductions of the
source term:
\begin{proposition}For all $s$ and $t$ in $\term$, if then $s \reduces t$ then
for all $T$ such that $\sound$, it is also true that $\sound[\sigma][s]$
\end{proposition}

The proof of soundness then mainly involves showing that the semantical counterparts
of the language's combinators we defined during the model construction are compatible
with the logical relation. Namely that e.g. if $\sound[\tyarrow][f][F]$ and
$\sound[\tylist][xs][XS]$ hold then it is also true that:
$\sound[\protect{\tylist[\tau]}][\temap(f ,xs)][\temap(F , XS)]$.

\begin{theorem}Given a term $t \colon \term$, a parallel substitution $\rho \colon \subst$
and an evaluation environment $R$ such that $\rho$ and $R$ are related ($\sounde$
holds), the evaluation of $t$ in $R$ is related to $t [\rho]$:
$\sound[\sigma][\protect{t[\rho]}][\termeval (t , R)][\Delta]$
\end{theorem}
\begin{proof}The theorem is proved by structural induction on the shape of the
typing derivation of $t$. The variable case is trivially discharged by using the
proof of $\sounde$.

All the other cases --except for the lambda one-- can be solved by combining
induction hypotheses with the appropriate lemma proving that the corresponding
semantical combinator respects the logical relation.

In the case where $t = \telam x. b$, we are given a context $E$ together with a
proof $inc$ that it is an extension of $\Delta$, a term $u$ and an object $U$
which are related $\sound[\sigma][u][U][E]$ and, finally, a term $s \colon \term[\tau][E]$
which reduces to $(\telam x. b) [\rho] \teapp u$.
First of all, we should notice that $s \reduces b [\rho , x \mapsto u]$
and therefore that to prove $\sound[\tau][s][T][E]$ it is enough to prove that
$\sound[\tau][\protect{b [\rho , x \mapsto u]}][T][E]$. And we get just that by
using the induction hypothesis with the related parallel substitution $\rho'$ and
evaluation environment $R'$ obtained by the combination of the weakening of $\rho$
(resp. $R$) along $inc$ with $u$ (resp. $U$).
\end{proof}

\begin{corollary}Given a term $t$, t reduces to its normal form: $t \reduces \norm t$.
And if two terms $t$ and $u$ have the same normal form up-to $\alpha$-equivalence
then they are indeed related: $t \conv u$.
\end{corollary}
\begin{proof}The identity parallel substitution is related to the diagonal evaluation
environment and $t [\idsubst]$ is equal to $t$ hence, by the previous theorem,
$\sound[\sigma][t][\termeval(t , \idmodele)]$ and then $t \reduces \norm t$.
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Completeness}

Completeness can be summed up by the fact that the interpretation of $\rules$
convertible elements produces semantical objects behaving similarly. This notion
of similar behaviour is formalized as \emph{semantic equality} where, in the
function case, we expect both sides to agree on any \emph{uniform} input rather
than any element of the model. As usual the list case is dealt with by using an
auxiliary definition parametric in its "interesting" arguments.

\begin{definition} The semantic equality of two elements $T, U$ of $\model$
is written $\exteq$ while $T \in \model$ being uniform is written $\uniform$.
They are both mutually defined by induction on the index $\sigma$ in the figure
page~\pageref{logreleq}.
\begin{figure*}
\begin{mathpar}
\inferrule{ }{\exteq[\tyunit]}
\and \inferrule{ }{\uniform[\tyunit]}

\and \inferrule{T = U}{\exteq[\tybase]}
\and \inferrule{ }{\uniform[\tybase]}

\and \inferrule{\exteq[\sigma][A][C] \\ \exteq[\tau][B][D]}{\exteq[\typrod][(A , B)][(C , D)]}
\and \inferrule{\uniform[\sigma][A] \\ \uniform[\tau][B]}{\uniform[\typrod][(A , B)]}

\and \inferrule{\forall \Delta (inc : \incl) (S : \model[\sigma][\Delta]) \rightarrow \uniform[\sigma][S] \rightarrow \exteq[\tau][F(\mathit{inc} , S)][G(\mathit{inc} , S)]}{\exteq[\tyarrow][F][G]}

\and \inferrule{\forall \Delta (inc : \incl), \uniform[\sigma][S] \rightarrow \uniform[\tau][F(inc , S)] \\
\\ \forall \Delta (\mathit{inc} : \incl) \rightarrow \uniform[\sigma][S_1] \rightarrow \uniform[\sigma][S_2] \rightarrow
\exteq[\sigma][S_1][S_2] \rightarrow \exteq[\tau][F(\mathit{inc} , S_1)][F(\mathit{inc} , S_2)] \\
\forall \mathit{inc_1}, \mathit{inc_2} \rightarrow \uniform[\sigma][S] \rightarrow
\exteq[\tau][\protect{\weaken[inc_1]~F(\mathit{inc_2} , S)}][\protect{F(\mathit{inc_2} \cdot \mathit{inc_1} , \weaken[inc_1]~ S)}]}
  {\uniform[\tyarrow][F]}

\end{mathpar}
\begin{mathpar}

\and \inferrule{ }{\tenil \colon \exteqlist[\sigma][\tenil][\tenil]}
\and \inferrule{ }{\uniform[\tylist][\tenil]}

\and \inferrule{\mathit{hd} \colon \mathit{EQ_\sigma} (X , Y) \\ \mathit{tl} \colon \exteqlist[\sigma][\mathit{XS}][\mathit{YS}]}
  {\mathit{hd} \tecons \mathit{tl} \colon \exteqlist[\sigma][X \tecons \mathit{XS}][Y \tecons \mathit{YS}]}
\and \inferrule{\uniform[\sigma][\mathit{HD}] \\ \uniform[\tylist][\mathit{TL}]}{\uniform[\tylist][\mathit{HD} \tecons \mathit{TL}]}

\and \inferrule{\mathit{xs} \colon \mathit{xs_1} \equiv \mathit{xs_2} \\
\mathit{YS} \colon \exteqlist[\sigma][\mathit{YS_1}][\mathit{YS_2}] \\
F \colon \forall \Delta (\mathit{inc} : \incl) (t : \neutral[\tau][\Delta]), \mathit{EQ_\sigma}(F_1 (\mathit{inc} , t) , F_2 (\mathit{inc} , t))}
  {\temap(F , \mathit{xs}) \teappend \mathit{YS} \colon \\ \exteqlist[\sigma][\temap(F_1 , \mathit{xs_1}) \teappend \mathit{YS_1}][\temap(F_2 , \mathit{xs_2}) \teappend \mathit{YS_2}]}

\and \inferrule{\forall \Delta (\mathit{inc} \colon \incl) (t \colon \neutral[\tau][\Delta]) , \uniform[\sigma][F(\mathit{inc} , t)] \\
\forall \mathit{inc_1}, \mathit{inc_2}, t, \exteq[\sigma][\protect{\weaken[inc_1]~F(\mathit{inc_2} , t)}][\protect{F(\mathit{inc_2} \cdot \mathit{inc_1} , \weaken[inc_1]~ t)}] \\
\uniform[\tylist][\mathit{YS}]}
{\uniform[\tylist][\temap(F , \mathit{xs}) \teappend \mathit{YS}]}
\end{mathpar}
\label{logreleq}
\caption{Semantic equality and uniformity of objects in the model}
\end{figure*}

Quite unsurprisingly, the unit case is of no interest: all the semantical units
are equivalent and uniform. Semantic equality for elements with base types is
up-to $\alpha$-equivalence: inhabitants are just bits of data (neutrals) which
can be compared in a purely syntactical fashion because we use nameless terms.
They are always uniform.

In the product case, the semantical objects are actual pairs and the definition
just forces the properties to hold for each one of the pair's components.

The function type case is a bit more hairy. While extensionality on uniform arguments is
simple to state, uniformity has to enforce a lot of invariants: application of uniform
objects should yield a uniform object, application of extensionally equal uniform objects
should yield extensionally equal objects and weakening and application should commute (up
to extensionality).

In the $\tylist$ case, extensional equality is an inductive set basically building
the (semantical) diagonal relation on lists of the same type. It is parametrized
by a relation $EQ_\sigma$ on terms of type $\model[\sigma][\Delta]$ (for any context
$\Delta$) which is, in the practical case instantiated with $\exteq[\sigma][.][.]$
as one would expect. Uniformity is, on the other hand, defined by recursion on the
semantical list. It could very well be defined as being parametric in something
behaving like $\uniform[\sigma][.]$ but this is not necessary: there are no positivity
problems! It is therefore probably better to stick to a lighter presentation here.
The empty list indeed is uniform. A constructor-headed list is said to be uniform
if its head of type $\model$ is uniform and its tail also is uniform. The criterion
for a stuck list is a bit more involved. Mimicking the definition of uniformity for
functions, there are two requirements on the stuck map: applying it to a neutral
yields a uniform element of the model and application and weakening commute.
Lastly the second argument of the stuck append should be uniform too.
\end{definition}

\begin{remark}The careful reader will already have noticed that this defines a
family of equivalence relations; we will not make explicit use of reflexivity,
symmetry and transitivity in the paper but it is fundamental in the formalization.
\end{remark}

Recall that the completeness theorem was presented as expressing the fact that
elements equivalent with respect to the reduction relation were interpreted as
semantical objects behaving similarly. For this approach to make sense, knowing
that two semantical objects are extensionally equal should immediately imply
that their respective reifications are syntactically equal. Which is the case.

\begin{lemma}Reification, reflection and weakenings are compatible with the
notions of extensional equality and uniformity.
\begin{enumerate}
  \item If $\exteq$ then $\reify T = \reify U$
  \item If $t_{ne}$ is a neutral $\neutral$ then $\uniform[\sigma][(\reflect t_{ne})]$
  \item Weakening and reification commute for uniform objects
\end{enumerate}
\end{lemma}

Now that we know that all the theorem proving ahead of us will not be meaningless,
we can start actually tackling completeness. When applying an extensional function,
it is always required to prove that the argument is uniform. Being able to certify
the uniformity of the evaluation of a term is therefore of the utmost importance.

\begin{lemma} Evaluation preserves properties of the evaluation environment.
\begin{enumerate}
  \item Evaluation in uniform environments produces uniform values
  \item Evaluation in semantically equivalent environments produces semantically equivalent values
  \item Weakening the evaluation of a term is equivalent to evaluating this term in a weakened environment
\end{enumerate}
\end{lemma}

\begin{theorem}If $s$ and $t$ are two terms in $\term$ such that $s \reduce t$
and if $R$ is a uniform environment in $\modele$ then
$\exteq[\sigma][\termeval(s , R)][\termeval(t , R)]$.
\end{theorem}
\begin{proof}One proceeds by induction on the proof that $s$ reduces to $t$.
\paragraph{Structural rules} The case of the structural rule for lambda can be
discharged quite simply by an induction hypothesis: indeed a weakened uniform
environment is still uniform and the element provided by the extensional equality
relation at an arrow type is assumed to be uniform.

The left structural rule for application is trivially discharged by combining the
induction hypothesis with the lemma guaranteeing that evaluation of terms in uniform
environments are uniform. The right structural one works the other way around:
the uniformity of the evaluation of the functional part precisely says that
application of uniform terms which are extensionally equal (induction hypothesis)
yiels semantically equal terms thus proving the goal.

The structure itself of the call graph of $\exteq$ on product types guarantees
that structural rules for pair formers can be discharged by a combination of
reflexivity and induction hypothesis while structural rules for projections are
taken care of by projecting the appropriate component of the induction hypothesis.

The structural rules for append, map and fold are dealt with by putting together
reflexivity proofs and the induction hypothesis using the proofs that these
semantical operations yield extensionally equal terms when fed with such kinds
of objects.

\paragraph{$\beta\iota$ rules} Each one the $\iota$ rules holds by reflexivity of
the extensional equality, indeed evaluation realizes these computation rules
syntactically. The case of the $\beta$ rule is slightly more complicated. Given a
function $\telam x. b$ and its argument $u$, one starts by proving that the diagonal
semantical environment extended with the evaluation of $u$ in $R$ is extensionally
equal to the evaluation in $R$ of the diagonal substitution extended with $u$.
Thence, knowing that the evaluations of a term in two extensionally equal
environments are extensionally equal, one can see that the evaluation of the
redex is related to the evaluation of the body in an environment corresponding
to the evaluation of the substitution generated when firing the redex. Finally,
the fact that $\termeval$ and substitution commute (up-to-extensionality) lets us
conclude.

\paragraph{$\eta\nu$ rules} definitely are the most complicated ones: except
for the ones for product and unit types which can be discharged by reflexivity of
the semantic equality, all of them need at least a little bit of theorem proving
to go through. The map-id, map-append, append-nil and append-assoc rules can be
proven using simple auxiliary lemmas proved by functional induction.
\end{proof}

\begin{corollary}[Completeness] For all terms $t$ and $u$ of type $\term$, if
$t \conv u$ then $\norm t = \norm u$.
\end{corollary}
\begin{proof}Reflection produces uniform values and uniformity is preserved
through weakening hence the fact that the trivial diagonal environment is uniform.
Combined with iterations of the previous lemma along the proof that $t \conv u$,
we get that the respective evaluations of $t$ and $u$ are extensionally equal which
we have proved to be enough to get syntactically equal reifications.
\end{proof}


\begin{corollary}The equational theory enriched with $\nu$-rules is decidable.
\end{corollary}
\begin{proof}Given terms $t$ and $u$ of the same type $\term$, we can get two normal
forms $t_{nf} = \norm t$ and $u_{nf} = \norm u$ and test them for equality up-to
$\alpha$-conversion (which is a simple syntactic check in our nameless representation
in \agda{}).

If $t_{nf} = u_{nf}$ then the soundness result allows us to conclude that $t$
and $u$ are convertible terms.

If $t_{nf} \neq u_{nf}$ then $t$ and $u$ are not convertible. Indeed, if they
were then the completeness result guarantees us that $t_{nf}$ and $u_{nf}$ would
be equal which leads to a contradiction.
\end{proof}

\begin{example} of terms which are identified thanks to the internalization of
the $\nu$-rules.
\begin{enumerate}
  \item In a context with two functions $f$ and $g$ of type $\tyarrow[\sigma][\tyunit]$,
  $\telam xs. \temap (f , xs)$ and $\telam xs. \temap (g , xs)$ both normalize to
  $\telam xs. \temap (\telam \_. \tett, xs) \teappend \tenil$ and are therefore
  declared equal.
  \item At type $\def\Two{(\typrod[\tybase][\protect{\tybase[l]}])} \def\Twos{\tylist[\Two]}\term[\protect{\tyarrow[\Twos][\Twos]}]$,
  the terms $\telam xs. xs$ and $\telam xs. \temap (swap , \temap (swap , xs))$
  where $swap$ is the function $\telam p. (\tepide p \tepair \tepiun p)$ swapping
  the order of a pair's elements are convertible with normal form
  $\telam xs. \temap (\telam p. (\tepiun p \tepair \tepide p) , xs) \teappend \tenil$.
\end{enumerate}
\end{example}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%% Future work %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section\protect{Further Opportunities for $\nu$-Rules}

We were motivated to develop a proof technique for extending
definitional equality with $\nu$-rules because there are many
opportunities where we might profit by doing so. Let us set out a
prospectus.

\paragraph{Reflexive coercion for type-based equality.} Altenkirch,
McBride and W. Swierstra developed a propositional equality for
intensional type theory~\cite{ObsEq} which differs from the usual inductive
definition ($\mathtt{refl~a~:~a~=~a}$) in that its main eliminator
\[
\inferrule{S,T : \mathtt{Set}\quad Q : S = T\quad s : S}
          {s[Q:S=T\rangle : T}
\]
computes by structural recursion first on the \emph{types}
$S$ and $T$, and then (where appropriate) on $s$, rather than
by pattern matching on the proof $Q$. Equality is still
reflexive, so evaluation can leave us with terms
$n[\mathtt{refl}\:n:N=N\rangle : N$
where $n$ is a neutral term in a neutral type $N$. It is clearly
a nuisance that this term does not compute to $n$, as would happen
if the eliminator matched on the proof. The fix is to add a
$\nu$-rule which discards coercions whenever it is type-safe to do so:
\[
  \fbb{\fba{$s$}$[Q:S=T\rangle$} = \fba{$s$}\qquad \mbox{if}\;S\equiv T:\mathtt{Set}
\]
It is easy to check that adding this rule for neutral terms makes it
admissible for all terms, and hence that we need add it not to
evaluation, but only to the reification process which follows, just
as with the $\nu$-rules in this paper. There, as here, this spares
the evaluation process from decisions which involve $\eta$-expansion
and thus require a name supply. The $\nu$-rule thus gives us a non-disruptive
means to respect the full computational behaviour of inductive equality
in the observational setting.

\paragraph{Functor laws.} Barral and Soloviev give a treatment of
functor laws for parametrized inductive datatypes by modifying the
$\iota$-rules of their underlying type
theory~\cite{DBLP:conf/csr/BarralS06}.  We should very much hope to
achieve the same result, as we did here in the special case of lists,
just by adding $\nu$-rules. Our preliminary
experiments~\cite{PigWeekNu} suggest that we can implement functor
laws once and for all in a type theory whose datatypes are given once
and for all by a syntactic encoding of strictly positive functors, as
Dagand and colleagues propose~\cite{epigram,ElabData}.  Moreover, Luo
and Adams have shown~\cite{DBLP:journals/mscs/LuoA08} that structural
subtyping for inductive types can be reified by a coherent system of
implicit coercions if functor laws hold definitionally.

\newcommand{\bind}{>\!\!>\!\!=}
\paragraph{Monad laws.} Watkins et al. give a definitional treatment
of monad laws in order to achieve
an adequate representation of concurrent processes encapsulated
monadically in a logical
framework~\cite{DBLP:conf/types/WatkinsCPW03}.
For straightforward free monads, an experimental extension of Epigram (by
Norell, as it happens)~\cite{PigWeekNu} suggests that we may readily allow
$\nu$-rules:
\[
\fbb{$\fba{$t$}\bind\mathtt{return}$} = \fba{$t$}
\qquad
\fbc{\(\fbb{\((\fba{$t$}\bind \sigma)\)}\bind\rho\)} =
\fbb{\(\fba{$t$}\bind((\bind \sigma)\cdot\rho)\)}
\]
Atkey's Foveran system uses a
similar normalization method for free monad laws~\cite{FusionMonad},
again for an encoded universe of underlying functors.

\paragraph{Decomposing functors.} Dagand and colleagues further
note that their syntax of descriptions for indexed functors is, by virtue of being
a syntax, itself presentable as the free monad of a functor. The
description decoder
\[
  \mathtt{Decode} : \mathtt{IDesc}\:I \to (I \to \mathtt{Set})
  \to \mathtt{Set}
\]
is structurally recursive in the description and lifts pointwise to
an interpretation of substitutions in the \(\mathtt{IDesc}\) monad
\[\begin{array}{l}
  \llbracket\_\rrbracket :
  (O \to \mathtt{IDesc}\:I) \;\;\to\;\;
     (I \to \mathtt{Set}) \to  (O \to \mathtt{Set}) \\
  \llbracket \sigma \rrbracket\:X\:o = \mathtt{Decode}\:(\sigma\:o)\:X
\end{array}\]
as indexed
functors with a `map' operation satisfying functor laws. However, not
only does this interpretation \emph{deliver} functors, it is
\emph{itself} a contravariant functor: the identity substitution yields
the identity functor just by $\beta\delta\iota$, but we may also
interpret Kleisli composition as reverse functor composition
\[
  \llbracket (>\!\!>\!\!=\sigma)\cdot\rho \rrbracket =
    \llbracket \rho \rrbracket \cdot \llbracket \sigma \rrbracket
\]
by means of a $\nu$-rule
\[
  \fbc{Decode~\fbb{$(\fba{$D$}>\!\!>\!\!=\sigma)$}~$X$} =
  \fbb{Decode~\fba{$D$}~$(\llbracket\sigma\rrbracket\:X)$}
\]
taking each $D$ to be some $\rho\:o$. If we want to do a `scrap your
boilerplate' style traversal of some described container-like
structure, we need merely exhibit the decomposition of the description as some
$(>\!\!>\!\!=\sigma)\cdot\rho$, where $\rho$ describes the invariant
superstructures and $\sigma$ the modified substructures, then invoke
the functoriality of $\llbracket \rho \rrbracket$. This $\nu$-rule
thus lets us expose functoriality over substructures not anticipated by explicit
parametrization in datatype declarations. We thus recover the kind of
ad hoc data traversal popularized by L\"ammel and Peyton
Jones~\cite{DBLP:conf/tldi/LammelJ03} by static structural means.

\paragraph{Universe embeddings.} A type theory with
inductive-recursive definitions is powerful enough to encode universes
of dependent types by giving a datatype of codes \emph{in tandem} with
their interpretations~\cite{DBLP:conf/tlca/DybjerS99}, the paradigmatic
example being
\[\begin{array}{@{}l@{\;}|@{\;}l@{}}
\mathtt{U}_1 : \mathtt{Set}
  & \mathtt{El}_1 : \mathtt{U}_1 \to \mathtt{Set} \\
\mathtt{`Pi}_1 : (S:\mathtt{U}_1)\to
  & \mathtt{El}_1\:(\mathtt{`Pi}_1\:S\:T) = \\
\qquad\qquad(\mathtt{El}_1\:S\to \mathtt{U}_1 )\to\mathtt{U}_1 &
\;\;(s : \mathtt{El}_1\:S)\to
  \mathtt{El}_1\:(T\:s) \\
\vdots & \vdots
\end{array}\]
Palmgren~\cite{Palmgren:universes} suggests that one way to model a
cumulative hierarchy of such universes is to give each a code in the
next, so
\[\begin{array}{@{}l@{\;}|@{\;}l@{}}
\mathtt{U}_2 : \mathtt{Set}
  & \mathtt{El}_2 : \mathtt{U}_2 \to \mathtt{Set} \\
\mathtt{`U}_1 : \mathtt{U}_2
  & \mathtt{El}_2\:\mathtt{`U}_1 = \mathtt{U}_1 \\
\mathtt{`Pi}_2 : (S:\mathtt{U}_2)\to
  & \mathtt{El}_2\:(\mathtt{`Pi}_2\:S\:T) = \\
\qquad\qquad(\mathtt{El}_2\:S\to \mathtt{U}_2 )\to\mathtt{U}_2 &
\;\;(s : \mathtt{El}_2\:S)\to
  \mathtt{El}_2\:(T\:s) \\
\vdots & \vdots
\end{array}\]
and then define an embedding recursively
\[\begin{array}{l}
\uparrow : \mathtt{U}_1 \to \mathtt{U}_2 \\
\uparrow(\mathtt{`Pi}_1\:S\:T) =
  \mathtt{`Pi}_2\:(\uparrow S)\:(\lambda s.\:\uparrow(T\:s))
\end{array}\]
but a small frustration with this proposal is that $s$ is abstracted at
type $\mathtt{El}_2\:(\uparrow S))$, but used at type
$\mathtt{El}_1\:S$, and these two types are not definitionally equal
for an abstract $S$. One workaround is to make $\uparrow$ a
constructor of $\mathtt{U}_2$, at the cost of some redunancy of
representation, but now we might also consider fixing the discrepancy
with a $\nu$-rule
\[
  \fbc{$\mathtt{El}_2\:\fbb{$(\uparrow \fba{$S$})$}) $} =
    \fbb{$\mathtt{El}_1\:\fba{$S$}$}
\]
This is peculiar for our examples thus far, in that the $\nu$-rule is
needed even to typecheck the $\delta\iota$-rules for $\uparrow$,
reflecting the fact that $\uparrow$ should not be any old function
from $\mathtt{U}_1$ to $\mathtt{U}_2$, but rather one which preserves
the meanings given by $\mathtt{El}_1$ and $\mathtt{El}_2$. In effect,
the $\nu$-rule is expressing the coherence property of a richer notion
of morphism. It is inviting to wonder what other notions of coherence
we might enable and enforce by checking that $\nu$-rules hold of the
operations we implement.

\paragraph{Non-examples.} A key characteristic of a $\nu$-rule is that
it is a nut-preserving rearrangement of neutral term layers. Whilst
this is good for associativity and sometimes for distributivity, it is
perfectly useless for commutativity. Suppose $+$ for natural numbers
is recursive on its first argument, and observe that rewriting $x + y$
to $y + x$ when $x$ is neutral will not result in a neutral term
unless $y$ is also neutral. Less ambitious rules such as
$x+\mathtt{suc}\:y = \mathtt{suc}\:(x+y)$ and $x*0 = 0$ similarly make
neutral terms come unstuck, and so cannot be postponed until
reification if we want to be sure that evaluation suffices to show
whether any expression in a datatype can be put into
constructor-headed form.



\section{Discussion}

We fully expect to be able to adapt this piece of technology to type theory without
deeply altering its evaluation algorithm. Andreas Abel and Peter Dybjer already
have proposed normalization by evaluation algorithms for various such systems,
working with either Klaus Aehlig~\cite{NbeDep1} or Thierry Coquand~\cite{NbeDep2}
and Nils Anders Danielsson formalized such an algorithm using an inductive-recursive
encoding~\cite{NadNbeDep}.

Finding good criterion for checking that some potential $\nu$-rules can be safely
added to the system is of the utmost importance. Indeed we want to move away from
the vision of definitional equality as being baked into the language once and
forall by letting the programmer pick new $\nu$-rules she demands to hold as long
as the machine is able to check that they are acceptable.

It could be interesting to figure out how to integrate these new reduction rules
to presentations of normalization other than big step semantics and normalization
by evaluation. For instance Gr\'{e}goire and Leroy's conversion by compilation to a
bytecode machine derived from Ocaml's ZAM~\cite{ConvTestZAMed} internalizes $\eta$
for functions by working on pairs of terms and only using this rule when a neutral
term faces a lambda abstraction; such a lazy approach to expansions is desirable
whenever possible but will not be tractable for all the equational theory (the
simplest counter-example being the addition of $\eta$ for unit types). Hereditary
substitutions, formalized by Abel~\cite{HeredSubst1} or Keller and
Altenkirch~\cite{HeredSubst2}, also comes to mind.

